# ğŸ§  RAG with Hexagonal Architecture (FastAPI + LLM + Vector DB)

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline featuring:

- **FastAPI** for the REST API layer
- **Multiple LLM providers** (Ollama, OpenAI, or litellm) for flexible language model integration
- **Qdrant** as the vector database for semantic similarity search
- **Hexagonal Architecture** with ports & adapters pattern ensuring clean separation of concerns
- **Domain-Driven Design (DDD)** principles for modeling complex business logic with a ubiquitous language
- **Layered architecture** with distinct application, domain, and infrastructure boundaries for enhanced modularity and testability

---

## ğŸ“ Project Structure
```
backend/
â”œâ”€â”€ config.py                  # Global configuration settings for the application
â”œâ”€â”€ logs/                      # Application logs directory
â”œâ”€â”€ main.py                    # Application entry point - initializes FastAPI and dependencies
â”œâ”€â”€ src/
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ rag/               # RAG component following hexagonal architecture
â”‚           â”œâ”€â”€ application/   # Application layer
â”‚           â”‚   â”œâ”€â”€ handlers/  # Use case handlers implementing business logic
â”‚           â”‚   â”‚              # (e.g., DocumentStoreHandler, QueryHandler)
â”‚           â”‚   â””â”€â”€ ports/     # Interfaces defining how to interact with the application core
â”‚           â”‚       â”œâ”€â”€ driven/  # Interfaces that the application uses to communicate outward
â”‚           â”‚       â”‚            # (e.g., EmbeddingPort, LLMPort, VectorStorePort)
â”‚           â”‚       â””â”€â”€ driving/ # Interfaces that allow external systems to use the application
â”‚           â”‚                    # (e.g., DocumentStorePort, QueryPort)
â”‚           â”œâ”€â”€ config/        # Component-specific configuration
â”‚           â”‚                  # (e.g., RAGConfig)
â”‚           â”œâ”€â”€ domain/        # Domain layer - core business rules and concepts
â”‚           â”‚   â”œâ”€â”€ entities/  # Business objects with identity and lifecycle
â”‚           â”‚   â”‚              # (e.g., QueryModel, RAGResponseModel)
â”‚           â”‚   â”œâ”€â”€ services/  # Domain services that operate on multiple entities
â”‚           â”‚   â”‚              # (e.g., DocumentStoreService, QueryService)
â”‚           â”‚   â””â”€â”€ value_objects/ # Immutable objects without identity
â”‚           â”‚                      # (e.g., Query, Embedding, Message, DocumentRetrieval)
â”‚           â”œâ”€â”€ infrastructure/ # Infrastructure layer - technical details and implementations
â”‚           â”‚   â”œâ”€â”€ adapters/  # Connect the application to external systems
â”‚           â”‚   â”‚   â”œâ”€â”€ driven/ # Implementations of ports the application uses
â”‚           â”‚   â”‚   â”‚           # (e.g., LiteLLMAdapter, DoclingAdapter)
â”‚           â”‚   â”‚   â””â”€â”€ driving/ # Implementations of ports to drive the application
â”‚           â”‚   â”œâ”€â”€ api/       # API definition and routing
â”‚           â”‚   â”‚   â”œâ”€â”€ di/    # Dependency Injection configuration
â”‚           â”‚   â”‚   â””â”€â”€ v1/    # API version 1
â”‚           â”‚   â”‚       â”œâ”€â”€ dto.py        # Data Transfer Objects for API requests/responses
â”‚           â”‚   â”‚       â””â”€â”€ rag_routes.py # FastAPI route definitions for RAG
â”‚           â”‚   â””â”€â”€ persistence/ # Repository implementations and data access
â”‚           â”‚                    # (e.g., QdrantVectorStoreAdapter, QdrantVectorRetrieverAdapter)
â”‚           â””â”€â”€ tests/         # Test files following the same structure as source code
â””â”€â”€ uvicorn_debug.py          # Script for running the app in debug mode with uvicorn
```

```
WIP: The project structure is subject to change as the project evolves.
```
![documentation/archi.png](documentation/archi.png)
---

## âš™ï¸ Key Concepts
- **Hexagonal Architecture**  
  Ports define *what the system needs*; adapters define *how it's fulfilled*.
  
- **RAG Process**
  1. A question is submitted to the API
  2. Vector search retrieves relevant documents
  3. LLM generates an answer based on those documents
  4. (Optionally) The interaction is logged in a database (WIP)

- **Swappable Adapters**
  - You can switch between OpenAI and Ollama LLM clients (WIP)
  - You can replace Qdrant with another vector store easily

---

## ğŸ Getting Started

1. Clone the repository
2. Install packages

```bash
docker compose up
make run
````

----

## ğŸ“Œ License

MIT â€“ feel free to use and adapt.
