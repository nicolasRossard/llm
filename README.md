# ğŸ§  RAG with Hexagonal Architecture (FastAPI + LLM + Vector DB)

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using:

- **FastAPI** for the API layer
- **Ollama or OpenAI** as the LLM provider
- **Qdrant** for vector similarity search
- A **hexagonal (ports & adapters)** architecture to ensure modularity, testability, and separation of concerns

---

## ğŸ“ Project Structure

```

rag\_with\_hex/
â”œâ”€â”€ application/
â”‚   â””â”€â”€ ports/
â”‚       â”œâ”€â”€ user\_side/
â”‚       â”‚   â”œâ”€â”€ usecase/                  # Use case interfaces (e.g., IAnswerQuestion)
â”‚       â”‚   â””â”€â”€ usecase\_handler/          # Implementations of use cases (business logic orchestration)
â”‚       â””â”€â”€ server\_side/
â”‚           â”œâ”€â”€ llm\_client.py             # Abstract interface (port) for LLM providers
â”‚           â””â”€â”€ vector\_search\_port.py     # Abstract interface (port) for vector search engines like Qdrant
â”‚
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ schema/                           # Domain models: Question, Answer, DocumentChunk (Pydantic)
â”‚   â””â”€â”€ service/
â”‚       â””â”€â”€ rag\_service.py                # Core RAG logic: retrieve context + generate answer
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ adapters/
â”‚       â”œâ”€â”€ user\_side/
â”‚       â”‚   â””â”€â”€ rest/
â”‚       â”‚       â”œâ”€â”€ rag\_routes.py         # FastAPI routes (controllers)
â”‚       â”‚       â”œâ”€â”€ dto.py                # Input/output Data Transfer Objects (DTOs)
â”‚       â”‚       â””â”€â”€ mapper.py             # Mapping between DTOs and domain models
â”‚       â”‚
â”‚       â””â”€â”€ server\_side/
â”‚           â”œâ”€â”€ llm\_openai/
â”‚           â”‚   â””â”€â”€ openai\_llm\_client\_adapter.py   # OpenAI implementation of LLM client port
â”‚           â”œâ”€â”€ llm\_ollama/
â”‚           â”‚   â””â”€â”€ ollama\_llm\_client\_adapter.py   # Ollama implementation of LLM client port
â”‚           â”œâ”€â”€ qdrant/
â”‚           â”‚   â””â”€â”€ qdrant\_adapter.py              # Qdrant implementation of vector search port
â”‚           â””â”€â”€ persistence/
â”‚               â””â”€â”€ log\_repository\_sqlite.py       # Storage adapter (example with SQLite)
â”‚
â”œâ”€â”€ common/
â”‚   â””â”€â”€ config.py                        # Shared configuration (constants, env variables, etc.)
â”‚
â””â”€â”€ main.py                              # Application entrypoint: sets up FastAPI, wires dependencies

````

---

## âš™ï¸ Key Concepts

- **Hexagonal Architecture**  
  Ports define *what the system needs*; adapters define *how it's fulfilled*.
  
- **RAG Process**
  1. A question is submitted to the API
  2. Vector search retrieves relevant documents
  3. LLM generates an answer based on those documents
  4. (Optionally) The interaction is logged in a database

- **Swappable Adapters**
  - You can switch between OpenAI and Ollama LLM clients
  - You can replace Qdrant with another vector store easily

---

## ğŸ Getting Started

1. Clone the repository
2. Install packages

```bash
docker compose up
make run
````

---

## ğŸ§ª Testing Without Real Qdrant

A fake implementation of `QdrantAdapter` is provided for local development/testing without connecting to a real vector DB.

---

## ğŸ“Œ License

MIT â€“ feel free to use and adapt.

```
