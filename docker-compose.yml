name: 'ollama-open-webui'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    ports:
      - 11434:11434
    volumes:
      - ./ollama_data:/root/.ollama
      - ./ollama_entrypoint.sh:/entrypoint.sh
    environment:
      - 'OLLAMA_MAX_LOADED_MODELS=4'
      - 'OLLAMA_NUM_PARALLEL=1'
      - 'OLLAMA_KEEP_ALIVE=-1'
    entrypoint: ["/entrypoint.sh"]
    deploy:
      resources:
        limits:
          memory: 10G
    networks:
      - llm_net

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    volumes:
      - ./open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 3008:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    networks:
      - llm_net

  qdrant:
    container_name: "rag_personal_vector_db"
    restart: on-failure
    image: qdrant/qdrant:v1.13.3
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage:z
    networks:
      - llm_net
networks:
  llm_net:
    external: true