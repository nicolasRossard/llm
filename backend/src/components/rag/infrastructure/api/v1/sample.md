<!-- image -->

## AU CÅ’UR DES MODÃˆLES CONVERSATIONNELS : LES RÃ‰SEAUX DE NEURONES

## RÃ©seau de neurones, kÃ©sako ?

Un rÃ©seau de neurones est un modÃ¨le reposant sur des hypothÃ¨ses statistiques et des rÃ¨gles, qui s'entraÃ®ne Ã  partir de grands volumes de donnÃ©es.  
Cet entraÃ®nement tend Ã  imiter celui d'un cerveau : les donnÃ©es fournies en entrÃ©e traversent plusieurs couches de Â« neurones Â» avant de fournir un rÃ©sultat.  

Chaque neurone contient des paramÃ¨tres, c'est-Ã -dire des nombres qui permettent, via des calculs simples, de dÃ©tecter des motifs rÃ©currents (ou *patterns*) dans les donnÃ©es d'entrÃ©e.  
Un algorithme, dit de **rÃ©tropropagation de l'erreur**, indique ensuite au rÃ©seau si le rÃ©sultat est juste afin qu'il puisse ajuster les paramÃ¨tres dans les neurones (cf. Figure 1).  

Cette opÃ©ration d'infÃ©rence puis de rÃ©tropropagation est effectuÃ©e de trÃ¨s grands nombres de fois sur de trÃ¨s nombreux exemples, afin d'obtenir de solides performances sur des nouvelles donnÃ©es.  
En ce sens, le modÃ¨le s'entraÃ®ne Ã  avoir raison aussi souvent que possible au regard de la distribution statistique des donnÃ©es d'entrÃ©e.

---

The image is a screenshot of a webpage. The page is titled *"Chaque fois deux carauses"* and is written in French.  
The main content of the page is divided into two sections.

### Section 1:
- **Title**: "Chaque fois deux carauses"
- **Content**:  
  - The first section is titled "Chaque fois deux carauses" and is written in French.  
  - It contains a list of four carauses, each with a unique number.  
  - The carauses are numbered 1 through 4.  

### Section 2:
- **Title**: "Motif abstract"
- **Content**:  
  - The second section is titled "Motif abstract" and is written in French.  
  - It contains a list of four carauses, each with a unique number.  
  - The carauses are numbered 5 through 10.  

<!-- image -->

### Sortie attendue

**Figure 1** : ReprÃ©sentation simplifiÃ©e de l'entraÃ®nement d'un rÃ©seau de neurones classifieur.  

Chaque couche dÃ©tecte des motifs rÃ©currents (ou *patterns*) Ã  partir de la couche prÃ©cÃ©dente. Plus les couches sont situÃ©es profondÃ©ment au sein du rÃ©seau, plus elles traitent des motifs abstraits.  

Les neurones se spÃ©cialisent par eux-mÃªmes : on ne prÃ©cise jamais que, pour diffÃ©rencier une voiture d'un chat, la prÃ©sence de roues est un indice.  
C'est cette spÃ©cialisation **sans supervision humaine** qui rend les rÃ©seaux de neurones difficilement interprÃ©tables.  

La structure d'un rÃ©seau de neurones est appelÃ©e **architecture**. Elle permet de reconstruire un rÃ©seau identique, en indiquant :
- le nombre de couches,  
- le nombre de neurones dans chaque couche,  
- et le type de ces neurones.  

Dans la Figure 1, l'architecture consiste en :
- une premiÃ¨re couche de trois neurones,  
- plusieurs couches non dÃ©taillÃ©es,  
- une avant-derniÃ¨re couche avec deux neurones,  
- et enfin une couche de rÃ©sultat avec un seul neurone.  

Cependant, imiter l'architecture d'un modÃ¨le ne permet pas toujours d'obtenir des rÃ©sultats comparables.

---

<!-- image -->

- En Europe, le collectif **BigScience**, composÃ© de plusieurs centaines de chercheurs, a entraÃ®nÃ© le LLM **BLOOM** sur le supercalculateur Jean-Zay de Paris-Saclay.  
Ce modÃ¨le est entraÃ®nÃ© Ã  rÃ©aliser les mÃªmes tÃ¢ches que GPT dans **46 langues naturelles** (y compris des langues rÃ©gionales ou en danger) et **13 langages de programmation**.  

Les ensembles de donnÃ©es utilisÃ©s pour l'entraÃ®nement sont tous disponibles en open-source, tout comme le modÃ¨le entraÃ®nÃ© via HuggingFace.  
Le modÃ¨le comprend **175 milliards de paramÃ¨tres**, soit autant que GPT-3.

---

### Table 1 : Exemples de LLM Ã  l'Ã©tat de l'art et des politiques d'ouverture associÃ©es

| ModÃ¨le                                 | AnnÃ©e       | Nb. maximal de paramÃ¨tres (en mds) | Architecture publique | ModÃ¨le entraÃ®nÃ© ouvert | DonnÃ©es ouvertes | Conversationnel (RLHF) | Accessible aux utilisateurs (UI ou API) |
|----------------------------------------|-------------|------------------------------------|-----------------------|------------------------|------------------|-------------------------|-----------------------------------------|
| BigScience BLOOM                       | 2022        | 175                                | âœ“                     | âœ“                      | âœ“                | âœ—                       | âœ“                                       |
| Google GLaM/PaLM                       | 2021 / 2022 | 1200 / 540                         | âœ“                     | âœ—                      | âœ—                | âœ—                       | âœ“ (API payante)                         |
| Google LaMDA/Bard                      | 2022        | 137 / ?                            | âœ“ / âœ—                 | âœ—                      | âœ—                | âœ“                       | âœ“ (UK/US)                               |
| Meta OPT                               | 2022        | 175                                | âœ“                     | âœ“                      | âœ“                | âœ—                       | âœ—                                       |
| Meta BlenderBot3                       | 2022        | 175                                | âœ“                     | âœ“                      | âœ“                | âœ“                       | âœ“ (US)                                  |
| Meta LLaMA                             | 2023        | 65                                 | âœ“                     | âœ“                      | âœ“                | âœ—                       | âœ—                                       |
| OpenAI GPT-3                           | 2020        | 175                                | âœ“                     | âœ—                      | âœ“ (1re version)  | âœ—                       | âœ“ (option payante)                      |
| OpenAI GPT-3.5 (InstructGPT / ChatGPT) | 2022        | 175 / ?                            | âœ“ / âœ—                 | âœ—                      | âœ—                | âœ“                       | âœ“ (option payante)                      |
| OpenAI GPT-4                           | 2023        | ?                                  | âœ—                     | âœ—                      | âœ—                | âœ“                       | âœ“ (API et UI payantes)                  |

---

## LLM CONVERSATIONNELS : UN CHANGEMENT DE PARADIGME POUR LES MOTEURS DE RECHERCHE

Les LLM ont dÃ©jÃ  mÃ©tamorphosÃ© les pratiques dans de nombreux domaines. Leur Ã©volution conversationnelle transformera Ã  son tour certains usages.  
Le marchÃ© des moteurs de recherche apparaÃ®t comme l'un des premiers secteurs impactÃ©s par ces dÃ©veloppements.  

La tÃ¢che d'un moteur de recherche consiste le plus souvent Ã  **ordonner des rÃ©sultats externes par pertinence** vis-Ã -vis d'une requÃªte fournie par un utilisateur.  

Ã€ l'origine, les moteurs de recherche Ã©taient d'autant plus efficaces que l'utilisateur connaissait leur fonctionnement.  
De nos jours, les acteurs du marchÃ© ont dÃ©veloppÃ© des outils en mesure de s'adapter Ã  tous les types d'utilisateurs.

---

ğŸ“„ RÃ©fÃ©rence : [BigScience BLOOM - Arxiv](https://arxiv.org/pdf/2211.05100.pdf)
